{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Chunking \n",
    "\n",
    "Separar el texto por las paginas del documento, para poder procesar cada una de ellas de manera independiente. Almacenando los chuncks (El texto por paginas y metadata) en un DataFrame de `polars`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies for Text Chunking\n",
    "\n",
    "%pip install polars\n",
    "%pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF text chunking\n",
    "\n",
    "La salida del proceso de OCR de UNSTRUTURED.io es una lista de diccionarios, donde cada diccionario contiene la metadata del elemento y el texto del mismo elemento, e.g.:\n",
    "\n",
    "```python\n",
    "data = [\n",
    "  {\n",
    "    \"type\": \"NarrativeText\",\n",
    "    \"element_id\": \"084bcfca09086336d78c8ba5c6103a13\",\n",
    "    \"text\": \"Cuando hablamos de capacitación en ingeniería de software, nos referimos:\",\n",
    "    \"metadata\": {\n",
    "      \"filetype\": \"application/pdf\",\n",
    "      \"languages\": [\n",
    "        \"eng\"\n",
    "      ],\n",
    "      \"page_number\": 1,\n",
    "      \"filename\": \"file.pdf\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"type\": \"NarrativeText\",\n",
    "    \"element_id\": \"f666bfbe7368828ec28fc42ca8724ce3\",\n",
    "    \"text\": \"A la mejora de la calidad de los procesos de desarrollo de software\\nb. A la mejora de la calidad de los productos de software\\nc. A la mejora de la calidad de los servicios de software\\nd. A la mejora de la calidad de los sistemas de software\", \n",
    "    \"metadata\": {\n",
    "      \"filetype\": \"application/pdf\",\n",
    "      \"languages\": [\n",
    "        \"eng\"\n",
    "      ],\n",
    "      \"page_number\": 2,\n",
    "      \"filename\": \"file.pdf\"\n",
    "    }\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "Para poder hacer un procesamiento del esta estructura, y crear el DataFrame de `polars` se necesita hacer un proceso de chunking, donde se separe el texto por paginas, y se cree una fila por cada pagina; con la siguiente función se puede hacer este proceso:\n",
    "\n",
    "```python\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import polars as pl\n",
    "\n",
    "def pdf_chunk(json_data: List[Dict]) -> pl.DataFrame:\n",
    "    # Agrupar los elementos por número de página\n",
    "    pages = {}\n",
    "    for item in json_data:\n",
    "        page_number = item['metadata']['page_number']\n",
    "        if page_number not in pages:\n",
    "            pages[page_number] = []\n",
    "        pages[page_number].append(item['text'])\n",
    "    \n",
    "    # Crear una lista de diccionarios con la estructura deseada\n",
    "    data = []\n",
    "    for page_number, texts in pages.items():\n",
    "        data.append({\n",
    "            'metadata': json.dumps({'page_number': page_number, 'filename': json_data[0]['metadata']['filename']}),\n",
    "            'text': ' '.join(texts)\n",
    "        })\n",
    "    \n",
    "    # Crear el DataFrame de Polars\n",
    "    return pl.DataFrame(data).with_row_index('id')\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "pdf_chunk(json.loads(data))\n",
    "```\n",
    "\n",
    "##### Output\n",
    "\n",
    "<div><style>\n",
    ".dataframe > thead > tr,\n",
    ".dataframe > tbody > tr {\n",
    "  text-align: right;\n",
    "  white-space: pre-wrap;\n",
    "}\n",
    "</style>\n",
    "<small>shape: (2, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>metadata</th><th>text</th></tr><tr><td>u32</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>0</td><td>&quot;{&quot;page_number&quot;: 1, &quot;filename&quot;:…</td><td>&quot;Cuando hablamos de capacitació…</td></tr><tr><td>1</td><td>&quot;{&quot;page_number&quot;: 2, &quot;filename&quot;:…</td><td>&quot;A la mejora de la calidad de l…</td></tr></tbody></table></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame en una Base de Datos SQLite\n",
    "\n",
    "Para poder almacenar el DataFrame en una base de datos SQLite, se puede usar las siguientes funciones, en donde `save_checkpoint()` guarda el DataFrame en la base de datos, y `load_checkpoint()` carga el DataFrame de la base de datos.:\n",
    "\n",
    "```python\n",
    "from typing import Optional\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "def save_checkpoint(df: pl.DataFrame, checkpoint_path: str, table_name: Optional[str] = 'ocr_data') -> None:\n",
    "    conn = sqlite3.connect(checkpoint_path)\n",
    "    temp_df = df.clone()\n",
    "    temp_df.drop_in_place('id')\n",
    "    temp_df.write_database(table_name=table_name, connection=f\"sqlite:///{checkpoint_path}\", if_table_exists=\"replace\")\n",
    "    conn.close()\n",
    "\n",
    "def load_checkpoint(df: pl.DataFrame, checkpoint_path: str, table_name: Optional[str] = 'ocr_data') -> pl.DataFrame:\n",
    "    conn = create_engine(f\"sqlite:///{checkpoint_path}\")\n",
    "    query = f\"SELECT * FROM {table_name}\"\n",
    "    df = pl.read_database(query=query, connection=conn.connect()).with_row_index('id')\n",
    "    return df\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clase TextChunk\n",
    "\n",
    "**Con la estructura que general el proceso de OCR de UNSTRUCTURED.io.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "import copy\n",
    "from typing import List, Dict, Optional, Union\n",
    "import polars as pl\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "class TextChunk():\n",
    "    def __init__(self, current_df: Optional[pl.DataFrame] = pl.DataFrame()):\n",
    "        self.current_df = current_df\n",
    "\n",
    "    def __pdf_chunk(self, json_data: List[Dict]) -> pl.DataFrame:\n",
    "        # Agrupar los elementos por número de página\n",
    "        pages = {}\n",
    "        for item in json_data:\n",
    "            page_number = item['metadata']['page_number']\n",
    "            if page_number not in pages:\n",
    "                pages[page_number] = []\n",
    "            pages[page_number].append(item['text'])\n",
    "        \n",
    "        # Crear una lista de diccionarios con la estructura deseada\n",
    "        data = []\n",
    "        for page_number, texts in pages.items():\n",
    "            data.append({\n",
    "                'metadata': json.dumps({'page_number': page_number, 'filetype': [0]['metadata']['filetype'], 'filename': json_data[0]['metadata']['filename']}),\n",
    "                'text': ' '.join(texts)\n",
    "            })\n",
    "        \n",
    "        # Crear el DataFrame de Polars\n",
    "        return pl.DataFrame(data).with_row_index('id', offset=len(self.current_df)+1)\n",
    "\n",
    "    def __rtf_chunk(self, json_data: List[Dict]) -> pl.DataFrame:\n",
    "        metadata = {\n",
    "            \"filetype\": json_data[0]['metadata']['filetype'], \n",
    "            \"filename\": json_data[0]['metadata']['filename']\n",
    "        }\n",
    "        text = []\n",
    "        for item in json_data:\n",
    "            text.append(item['text'])\n",
    "        data = {\n",
    "            'metadata': json.dumps(metadata),\n",
    "            'text': ' '.join(text)\n",
    "        }\n",
    "        # Crear el DataFrame de Polars\n",
    "        return pl.DataFrame(data).with_row_index('id', offset=len(self.current_df)+1) \n",
    "\n",
    "    def __add_if_not_exists(self, new_data: Union[pl.DataFrame, Dict], key_columns: Optional[List]=None) -> pl.DataFrame:\n",
    "        if key_columns is None:\n",
    "            key_columns = ['metadata', 'text']\n",
    "        # Si nuevos_datos es un diccionario, convertirlo a DataFrame\n",
    "        if isinstance(new_data, dict):\n",
    "            new_data = pl.DataFrame([new_data])\n",
    "        if not isinstance(new_data, pl.DataFrame):\n",
    "            raise TypeError(\"nuevos_datos debe ser un DataFrame de Polars o un diccionario\")\n",
    "        if self.current_df.is_empty():\n",
    "            self.current_df = self.current_df.vstack(new_data)\n",
    "            return self.current_df\n",
    "        # Crear una expresión para verificar si los datos ya existen\n",
    "        condition = pl.all_horizontal([\n",
    "            pl.col(col).is_in(new_data[col])\n",
    "            for col in key_columns\n",
    "        ])\n",
    "        # Filtrar los datos existentes\n",
    "        existing_data = self.current_df.filter(condition)\n",
    "        # Identificar los datos nuevos\n",
    "        new = new_data.join(\n",
    "            existing_data.select(key_columns),\n",
    "            on=key_columns,\n",
    "            how=\"anti\"\n",
    "        )\n",
    "        # Si hay datos nuevos, agregarlos al DataFrame original\n",
    "        if not new.is_empty():\n",
    "            print(\"Se han encontrado datos nuevos para agregar\")\n",
    "            self.current_df = pl.concat([self.current_df, new], how=\"vertical\")\n",
    "        else:\n",
    "            print(\"No hay datos nuevos para agregar\")\n",
    "        return self.current_df\n",
    "    \n",
    "    def text_chunks_to_dataframe(self, json_data: List[Dict]) -> pl.DataFrame:\n",
    "        filetype = json_data[0]['metadata']['filetype']\n",
    "        if filetype == \"application/pdf\":\n",
    "            df = self.__pdf_chunk(json_data)\n",
    "        elif filetype == \"text/rtf\":\n",
    "            df = self.__rtf_chunk(json_data)\n",
    "        elif filetype.startswith('text'):\n",
    "            data = copy.deepcopy(json_data)\n",
    "            data[0]['metadata'] = json.dumps(data[0]['metadata'])\n",
    "            df = pl.DataFrame(data).with_row_index('id', offset=len(self.current_df)+1)\n",
    "\n",
    "        self.__add_if_not_exists(new_data=df)\n",
    "        return self.current_df\n",
    "\n",
    "    def save_checkpoint(self, checkpoint_path: str, table_name: Optional[str] = 'ocr_data') -> None:\n",
    "        conn = sqlite3.connect(checkpoint_path)\n",
    "        temp_df = self.current_df.clone()\n",
    "        temp_df.drop_in_place('id')\n",
    "        temp_df.write_database(table_name=table_name, connection=f\"sqlite:///{checkpoint_path}\", if_table_exists=\"replace\")\n",
    "        conn.close()\n",
    "    \n",
    "    def load_checkpoint(self, checkpoint_path: str, table_name: Optional[str] = 'ocr_data') -> pl.DataFrame:\n",
    "        conn = create_engine(f\"sqlite:///{checkpoint_path}\")\n",
    "        query = f\"SELECT * FROM {table_name}\"\n",
    "        self.current_df = pl.read_database(query=query, connection=conn.connect()).with_row_index('id')\n",
    "        return self.current_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Clase TextChunk\n",
    "\n",
    "**Con la capa de OCR que se genera con ayuda de OCRmyPDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "import sqlite3\n",
    "from typing import List, Dict, Optional, Union\n",
    "from sqlalchemy import create_engine\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "class TextChunk():\n",
    "    \"\"\"\n",
    "    Class to handle text chunks and add them to a Polars DataFrame\n",
    "    \"\"\"\n",
    "    def __init__(self, current_df: Optional[pl.DataFrame] = pl.DataFrame()):\n",
    "        self.current_df = current_df\n",
    "    \n",
    "    def _pdf_chunk(self, json_data: List[Dict]) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a DataFrame from a list of dictionaries with text chunks from a PDF file\n",
    "\n",
    "        Args:\n",
    "            json_data (List[Dict]): List of dictionaries with text chunks from a PDF file\n",
    "        \n",
    "        Returns:\n",
    "            pl.DataFrame: polars DataFrame with the text chunks\n",
    "        \"\"\"\n",
    "        for item in json_data:\n",
    "            item['metadata'] = json.dumps(item['metadata'])\n",
    "            item['text'] = item['text'].decode('utf-8')\n",
    "        # Crea el DataFrame de Polars con los datos y un índice que empieza en el último índice del DataFrame actual\n",
    "        return pl.DataFrame(json_data).with_row_index('id', offset=len(self.current_df)+1)\n",
    "\n",
    "    def _add_if_not_exists(self, new_data: Union[pl.DataFrame, Dict], key_columns: Optional[List] = ['metadata', 'text']) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Add new data to the current DataFrame if it does not exist\n",
    "\n",
    "        Args:\n",
    "            new_data (Union[pl.DataFrame, Dict]): New data to add to the DataFrame\n",
    "            key_columns (Optional[List]): Columns to use as keys to identify if the data already exists, by default ['metadata', 'text']\n",
    "        \n",
    "        Returns:\n",
    "            pl.DataFrame: Currently updated polars DataFrame\n",
    "        \"\"\"\n",
    "        # Si nuevos_datos es un diccionario, convertirlo a DataFrame, si no, verificar que sea un DataFrame\n",
    "        if isinstance(new_data, dict):\n",
    "            new_data = pl.DataFrame([new_data])\n",
    "        elif not isinstance(new_data, pl.DataFrame):\n",
    "            raise TypeError(\"nuevos_datos debe ser un DataFrame de Polars o un diccionario\")\n",
    "        # Si el DataFrame actual está vacío, asignarle los nuevos datos y retornarlo\n",
    "        if self.current_df.is_empty():\n",
    "            self.current_df = self.current_df.vstack(new_data)\n",
    "            return self.current_df\n",
    "        # Crear una expresión para verificar si los datos ya existen\n",
    "        condition = pl.all_horizontal([\n",
    "            pl.col(col).is_in(new_data[col])\n",
    "            for col in key_columns\n",
    "        ])\n",
    "        # Filtrar los datos existentes\n",
    "        existing_data = self.current_df.filter(condition)\n",
    "        # Identificar los datos nuevos\n",
    "        new = new_data.join(\n",
    "            existing_data.select(key_columns),\n",
    "            on=key_columns,\n",
    "            how=\"anti\"\n",
    "        )\n",
    "        # Si hay datos nuevos, agregarlos al DataFrame original\n",
    "        if not new.is_empty():\n",
    "            print(\"Se han encontrado datos nuevos para agregar\")\n",
    "            self.current_df = pl.concat([self.current_df, new], how=\"vertical\")\n",
    "        else:\n",
    "            print(\"No hay datos nuevos para agregar\")\n",
    "        return self.current_df\n",
    "    \n",
    "    def add_chunks_to_dataframe(self, json_data: List[Dict]) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Add text chunks to the current DataFrame. The method will identify the type of file and call the corresponding method to process the data\n",
    "\n",
    "        Args:\n",
    "            json_data (List[Dict]): List of dictionaries with text chunks\n",
    "        \n",
    "        Returns:\n",
    "            pl.DataFrame: Currently updated polars DataFrame\n",
    "        \"\"\"\n",
    "        filetype = json_data[0]['metadata']['filetype']\n",
    "        if filetype == \"application/pdf\":\n",
    "            df = self._pdf_chunk(json_data)\n",
    "        elif filetype.startswith('text'):\n",
    "            data = copy.deepcopy(json_data)\n",
    "            data[0]['metadata'] = json.dumps(data[0]['metadata'])\n",
    "            df = pl.DataFrame(data).with_row_index('id', offset=len(self.current_df)+1)\n",
    "        self._add_if_not_exists(new_data=df)\n",
    "        return self.current_df\n",
    "\n",
    "    def save_checkpoint(self, checkpoint_path: str, table_name: Optional[str] = 'ocr_data') -> None:\n",
    "        \"\"\"\n",
    "        Save the current DataFrame to a SQLite database\n",
    "\n",
    "        Args:\n",
    "            checkpoint_path (str): Path to the SQLite database\n",
    "            table_name (Optional[str]): Name of the table to store the data, by default 'ocr_data'\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        conn = sqlite3.connect(checkpoint_path)\n",
    "        temp_df = self.current_df.clone()\n",
    "        temp_df.drop_in_place('id')\n",
    "        temp_df.write_database(table_name=table_name, connection=f\"sqlite:///{checkpoint_path}\", if_table_exists=\"replace\")\n",
    "        conn.close()\n",
    "    \n",
    "    def load_checkpoint(self, checkpoint_path: str, table_name: Optional[str] = 'ocr_data') -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Load a DataFrame from a SQLite database\n",
    "\n",
    "        Args:\n",
    "            checkpoint_path (str): Path to the SQLite database\n",
    "            table_name (Optional[str]): Name of the table to load the data from, by default 'ocr_data'\n",
    "\n",
    "        Returns:\n",
    "            pl.DataFrame: DataFrame loaded from the SQLite database (current DataFrame)\n",
    "        \"\"\"\n",
    "        conn = create_engine(f\"sqlite:///{checkpoint_path}\")\n",
    "        query = f\"SELECT * FROM {table_name}\"\n",
    "        self.current_df = pl.read_database(query=query, connection=conn.connect()).with_row_index('id')\n",
    "        return self.current_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo de uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This PDF is marked as a Tagged PDF. This often indicates that the PDF was generated from an office document and does not need OCR. PDF pages processed by OCRmyPDF may not be tagged correctly.\n",
      "Start processing 2 pages concurrently\n",
      "    1 page already has text! - rasterizing text and running OCR anyway\n",
      "    2 page already has text! - rasterizing text and running OCR anyway\n",
      "Postprocessing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR aplicado exitosamente a /Users/jorge-jrzz/Desktop/chunks/API/dev/example.pdf. Salida: /Users/jorge-jrzz/Desktop/chunks/API/dev/example.pdf\n",
      "shape: (2, 3)\n",
      "┌─────┬─────────────────────────────────┬─────────────────────────────────┐\n",
      "│ id  ┆ metadata                        ┆ text                            │\n",
      "│ --- ┆ ---                             ┆ ---                             │\n",
      "│ u32 ┆ str                             ┆ str                             │\n",
      "╞═════╪═════════════════════════════════╪═════════════════════════════════╡\n",
      "│ 1   ┆ {\"filetype\": \"application/pdf\"… ┆ Titulo                          │\n",
      "│     ┆                                 ┆ Encabezado 1                    │\n",
      "│     ┆                                 ┆ Para empez…                     │\n",
      "│ 2   ┆ {\"filetype\": \"application/pdf\"… ┆ Esto es texto que esta en una … │\n",
      "└─────┴─────────────────────────────────┴─────────────────────────────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image optimization did not improve the file - optimizations will not be used\n",
      "Image optimization ratio: 1.00 savings: -0.0%\n",
      "Total file size ratio: 1.00 savings: 0.0%\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./..')\n",
    "\n",
    "from core import OCR\n",
    "\n",
    "data = OCR.get_ocr(\"example.pdf\")\n",
    "text_chunk = TextChunk()\n",
    "df = text_chunk.add_chunks_to_dataframe(data)\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chunks-nbCgvLYC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
