{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking Text\n",
    "\n",
    "Separar el texto por las paginas del documento, para poder procesar cada una de ellas de manera independiente. Almacenando los chuncks (El texto por paginas y metadata) en un DataFrame de `polars`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install polars\n",
    "%pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "file = Path(\"salida.txt\")\n",
    "data = file.read_text(encoding='utf-8')\n",
    "data = data.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>metadata</th><th>text</th></tr><tr><td>u32</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>0</td><td>&quot;{&quot;page_number&quot;: 1, &quot;filename&quot;:…</td><td>&quot;18. Cuando hablamos de capacit…</td></tr><tr><td>1</td><td>&quot;{&quot;page_number&quot;: 2, &quot;filename&quot;:…</td><td>&quot;24. Son las etapas del procedi…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 3)\n",
       "┌─────┬─────────────────────────────────┬─────────────────────────────────┐\n",
       "│ id  ┆ metadata                        ┆ text                            │\n",
       "│ --- ┆ ---                             ┆ ---                             │\n",
       "│ u32 ┆ str                             ┆ str                             │\n",
       "╞═════╪═════════════════════════════════╪═════════════════════════════════╡\n",
       "│ 0   ┆ {\"page_number\": 1, \"filename\":… ┆ 18. Cuando hablamos de capacit… │\n",
       "│ 1   ┆ {\"page_number\": 2, \"filename\":… ┆ 24. Son las etapas del procedi… │\n",
       "└─────┴─────────────────────────────────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from typing import List, Dict\n",
    "import polars as pl\n",
    "\n",
    "def pdf_chunk(json_data: List[Dict]) -> pl.DataFrame:\n",
    "    # Agrupar los elementos por número de página\n",
    "    pages = {}\n",
    "    for item in json_data:\n",
    "        page_number = item['metadata']['page_number']\n",
    "        if page_number not in pages:\n",
    "            pages[page_number] = []\n",
    "        pages[page_number].append(item['text'])\n",
    "    \n",
    "    # Crear una lista de diccionarios con la estructura deseada\n",
    "    data = []\n",
    "    for page_number, texts in pages.items():\n",
    "        data.append({\n",
    "            'metadata': json.dumps({'page_number': page_number, 'filename': json_data[0]['metadata']['filename']}),\n",
    "            'text': ' '.join(texts)\n",
    "        })\n",
    "    \n",
    "    # Crear el DataFrame de Polars\n",
    "    return pl.DataFrame(data).with_row_index('id')\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "pdf_chunk(json.loads(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame en una Base de Datos SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>metadata</th><th>text</th></tr><tr><td>u32</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>0</td><td>&quot;{&quot;page_number&quot;: 1, &quot;filename&quot;:…</td><td>&quot;18. Cuando hablamos de capacit…</td></tr><tr><td>1</td><td>&quot;{&quot;page_number&quot;: 2, &quot;filename&quot;:…</td><td>&quot;24. Son las etapas del procedi…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 3)\n",
       "┌─────┬─────────────────────────────────┬─────────────────────────────────┐\n",
       "│ id  ┆ metadata                        ┆ text                            │\n",
       "│ --- ┆ ---                             ┆ ---                             │\n",
       "│ u32 ┆ str                             ┆ str                             │\n",
       "╞═════╪═════════════════════════════════╪═════════════════════════════════╡\n",
       "│ 0   ┆ {\"page_number\": 1, \"filename\":… ┆ 18. Cuando hablamos de capacit… │\n",
       "│ 1   ┆ {\"page_number\": 2, \"filename\":… ┆ 24. Son las etapas del procedi… │\n",
       "└─────┴─────────────────────────────────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Optional\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "def save_checkpoint(df: pl.DataFrame, checkpoint_path: str, table_name: Optional[str] = 'ocr_data') -> None:\n",
    "    conn = sqlite3.connect(checkpoint_path)\n",
    "    temp_df = df.clone()\n",
    "    temp_df.drop_in_place('id')\n",
    "    temp_df.write_database(table_name=table_name, connection=f\"sqlite:///{checkpoint_path}\", if_table_exists=\"replace\")\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def load_checkpoint(df: pl.DataFrame, checkpoint_path: str, table_name: Optional[str] = 'ocr_data') -> pl.DataFrame:\n",
    "    conn = create_engine(f\"sqlite:///{checkpoint_path}\")\n",
    "    query = f\"SELECT * FROM {table_name}\"\n",
    "    df = pl.read_database(query=query, connection=conn.connect()).with_row_index('id')\n",
    "    return df\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "df = pdf_chunk(json.loads(data))\n",
    "checkpoint_path = \"checkpoint.db\"\n",
    "save_checkpoint(df, checkpoint_path)\n",
    "df = load_checkpoint(df, checkpoint_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clase TextChunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, sqlite3, copy\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Union\n",
    "import polars as pl\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "class TextChunk():\n",
    "\n",
    "    def __init__(self, current_df: Optional[pl.DataFrame] = pl.DataFrame()):\n",
    "        self.current_df = current_df\n",
    "\n",
    "    def __pdf_chunk(self, json_data: List[Dict]) -> pl.DataFrame:\n",
    "        # Agrupar los elementos por número de página\n",
    "        pages = {}\n",
    "        for item in json_data:\n",
    "            page_number = item['metadata']['page_number']\n",
    "            if page_number not in pages:\n",
    "                pages[page_number] = []\n",
    "            pages[page_number].append(item['text'])\n",
    "        \n",
    "        # Crear una lista de diccionarios con la estructura deseada\n",
    "        data = []\n",
    "        for page_number, texts in pages.items():\n",
    "            data.append({\n",
    "                'metadata': json.dumps({'page_number': page_number, 'filename': json_data[0]['metadata']['filename']}),\n",
    "                'text': ' '.join(texts)\n",
    "            })\n",
    "        \n",
    "        # Crear el DataFrame de Polars\n",
    "        return pl.DataFrame(data).with_row_index('id', offset=len(self.current_df)+1)\n",
    "    \n",
    "\n",
    "    def __rtf_chunk(self, json_data: List[Dict]) -> pl.DataFrame:\n",
    "        metadata = {\n",
    "            \"filetype\": json_data[0]['metadata']['filetype'], \n",
    "            \"filename\": json_data[0]['metadata']['filename']\n",
    "        }\n",
    "        text = []\n",
    "\n",
    "        for item in json_data:\n",
    "            text.append(item['text'])\n",
    "\n",
    "        data = {\n",
    "            'metadata': json.dumps(metadata),\n",
    "            'text': ' '.join(text)\n",
    "        }\n",
    "\n",
    "        # Crear el DataFrame de Polars\n",
    "        return pl.DataFrame(data).with_row_index('id', offset=len(self.current_df)+1) \n",
    "\n",
    "    def __add_if_not_exists(self, new_data: Union[pl.DataFrame, Dict], key_columns: Optional[List]=None) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Agrega nuevas filas al DataFrame si no existen basándose en columnas clave.\n",
    "        \n",
    "        :param df: DataFrame de Polars existente\n",
    "        :param nuevos_datos: DataFrame o diccionario con los nuevos datos\n",
    "        :param columnas_clave: Lista de nombres de columnas para verificar la existencia\n",
    "        :return: DataFrame actualizado\n",
    "        \"\"\"\n",
    "        if key_columns is None:\n",
    "            key_columns = ['metadata', 'text']\n",
    "        # Si nuevos_datos es un diccionario, convertirlo a DataFrame\n",
    "        if isinstance(new_data, dict):\n",
    "            new_data = pl.DataFrame([new_data])\n",
    "        if not isinstance(new_data, pl.DataFrame):\n",
    "            raise TypeError(\"nuevos_datos debe ser un DataFrame de Polars o un diccionario\")\n",
    "        \n",
    "        if self.current_df.is_empty():\n",
    "            self.current_df = self.current_df.vstack(new_data)\n",
    "            return self.current_df\n",
    "\n",
    "        # Crear una expresión para verificar si los datos ya existen\n",
    "        condition = pl.all_horizontal([\n",
    "            pl.col(col).is_in(new_data[col])\n",
    "            for col in key_columns\n",
    "        ])\n",
    "\n",
    "        # Filtrar los datos existentes\n",
    "        existing_data = self.current_df.filter(condition)\n",
    "\n",
    "        # Identificar los datos nuevos\n",
    "        new = new_data.join(\n",
    "            existing_data.select(key_columns),\n",
    "            on=key_columns,\n",
    "            how=\"anti\"\n",
    "        )\n",
    "\n",
    "        # Si hay datos nuevos, agregarlos al DataFrame original\n",
    "        if not new.is_empty():\n",
    "            print(\"Se han encontrado datos nuevos para agregar\")\n",
    "            self.current_df = pl.concat([self.current_df, new], how=\"vertical\")\n",
    "        else:\n",
    "            print(\"No hay datos nuevos para agregar\")\n",
    "        return self.current_df\n",
    "    \n",
    "\n",
    "    def text_chunks_to_dataframe(self, json_data: List[Dict]) -> pl.DataFrame:\n",
    "        filetype = json_data[0]['metadata']['filetype']\n",
    "        if filetype == \"application/pdf\":\n",
    "            df = self.__pdf_chunk(json_data)\n",
    "        elif filetype == \"text/rtf\":\n",
    "            df = self.__rtf_chunk(json_data)\n",
    "        elif filetype.startswith('text'):\n",
    "            data = copy.deepcopy(json_data)\n",
    "            data[0]['metadata'] = json.dumps(data[0]['metadata'])\n",
    "            df = pl.DataFrame(data).with_row_index('id', offset=len(self.current_df)+1)\n",
    "\n",
    "        self.__add_if_not_exists(new_data=df)\n",
    "        return self.current_df\n",
    "\n",
    "\n",
    "    def save_checkpoint(self, checkpoint_path: str, table_name: Optional[str] = 'ocr_data') -> None:\n",
    "        \"\"\"\n",
    "        Save the current DataFrame to a SQLite database checkpoint.\n",
    "\n",
    "        This method saves the current DataFrame to a SQLite database checkpoint file. If the file already exists, it\n",
    "        will be overwritten.\n",
    "\n",
    "        Parameters:\n",
    "            checkpoint_path (str): The path to the SQLite database checkpoint file.\n",
    "        \"\"\"\n",
    "\n",
    "        conn = sqlite3.connect(checkpoint_path)\n",
    "        temp_df = self.current_df.clone()\n",
    "        temp_df.drop_in_place('id')\n",
    "        temp_df.write_database(table_name=table_name, connection=f\"sqlite:///{checkpoint_path}\", if_table_exists=\"replace\")\n",
    "        conn.close()\n",
    "    \n",
    "    \n",
    "    def load_checkpoint(self, checkpoint_path: str, table_name: Optional[str] = 'ocr_data') -> pl.DataFrame:\n",
    "        conn = create_engine(f\"sqlite:///{checkpoint_path}\")\n",
    "        query = f\"SELECT * FROM {table_name}\"\n",
    "        self.current_df = pl.read_database(query=query, connection=conn.connect()).with_row_index('id')\n",
    "        return self.current_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No hay datos nuevos para agregar\n",
      "Se han encontrado datos nuevos para agregar\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>metadata</th><th>text</th></tr><tr><td>u32</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>0</td><td>&quot;{&quot;page_number&quot;: 1, &quot;filename&quot;:…</td><td>&quot;18. Cuando hablamos de capacit…</td></tr><tr><td>1</td><td>&quot;{&quot;page_number&quot;: 2, &quot;filename&quot;:…</td><td>&quot;24. Son las etapas del procedi…</td></tr><tr><td>2</td><td>&quot;{&quot;filename&quot;: &quot;archivo.txt&quot;, &quot;f…</td><td>&quot;Hola, este es un archivo de te…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 3)\n",
       "┌─────┬─────────────────────────────────┬─────────────────────────────────┐\n",
       "│ id  ┆ metadata                        ┆ text                            │\n",
       "│ --- ┆ ---                             ┆ ---                             │\n",
       "│ u32 ┆ str                             ┆ str                             │\n",
       "╞═════╪═════════════════════════════════╪═════════════════════════════════╡\n",
       "│ 0   ┆ {\"page_number\": 1, \"filename\":… ┆ 18. Cuando hablamos de capacit… │\n",
       "│ 1   ┆ {\"page_number\": 2, \"filename\":… ┆ 24. Son las etapas del procedi… │\n",
       "│ 2   ┆ {\"filename\": \"archivo.txt\", \"f… ┆ Hola, este es un archivo de te… │\n",
       "└─────┴─────────────────────────────────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo de uso\n",
    "\n",
    "other_data = [{'metadata': {'filename': 'archivo.txt', 'filetype': 'text/txt'},\n",
    "  'text': 'Hola, este es un archivo de texto.'}]\n",
    "\n",
    "text = TextChunk()\n",
    "text.text_chunks_to_dataframe(json.loads(data))\n",
    "text.text_chunks_to_dataframe(json.loads(data))\n",
    "text.text_chunks_to_dataframe(other_data)\n",
    "text.save_checkpoint(\"checkpoint.db\")\n",
    "text.load_checkpoint(\"checkpoint.db\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chunks-nbCgvLYC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
